---
title: Loss Functions (II)
date: 2023-10-23
slides: https://moodle.bath.ac.uk/mod/resource/view.php?id=1262146
---

## Binary Classification

Here, we need a probability distribution $\Pr\big(\mathbf y \big| \boldsymbol\theta\big)$ which handles our domain $y \in \big\{0, 1\big\}$ and has parameters $\boldsymbol\theta$.

The most obvious choice is a [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) here, which accepts a parameter $\lambda$:

$$\begin{align*}
    \Pr\big(y | \lambda\big) &= \begin{cases}
        1 - \lambda & y = 0\\
        \lambda & y = 1\\
    \end{cases}\\
    \Pr\big(y | \lambda\big) &= (1 - \lambda)^{1-y} \cdot \lambda^y
\end{align*}$$

Great! But now what happens when we hook up the output of our neural network &mdash; we could literally have any output!

Our loss function only accepts $\lambda \in [0,1]$. We need a function that maps $\R \rightarrow [0,1]$. Enter, the sigmoid function:

### The Sigmoid Function

$$\mathrm{sig}\left[z\right] = \frac{1}{1+\exp\left[-z\right]}$$

Notice here, that:
$$\begin{align*}
    \lim_{z\rightarrow+\infty} \mathrm{sig}\left[z\right] &= 1\\
    \lim_{z\rightarrow-\infty} \mathrm{sig}\left[z\right] &= 0\\
    \mathrm{sig}\left[0\right] &= 0.5
\end{align*}$$
Now, we can modify our machine learning model with parameters $\boldsymbol \theta = \bold f\left[\bold x, \boldsymbol{ \phi}\right]$:

$$
\Pr\left(y\ |\ \bold x\right) = \left(1 - \mathrm{sig}\Big[\mathrm f \big[\bold x\ | \boldsymbol{\theta}\big]\Big]\right)^{1-y} \cdot \mathrm{sig}\Big[\mathrm f \big[\bold x\ | \boldsymbol{\theta}\big]\Big]^y
$$
